{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temasek Polytechnic  \n",
    "Specialist Diploma in Big Data Management  \n",
    "Big Data Programming (CBG1C04)  \n",
    "Assignment  \n",
    "  \n",
    "By Lup Peng \n",
    "  \n",
    "----------------------------------------\n",
    "### Overview \n",
    "The main datasets used for this project are the Austin B-Cycle Trips, and Kiosk Locations. They were downloaded from data.austintexas.gov, the official city of Austin open data portal.\n",
    "\n",
    "A synthetic data set of B-Cycle Customer Data for Memberships has been generated as well.\n",
    "\n",
    "#### Dataset Links\n",
    "- https://data.austintexas.gov/dataset/Austin-B-Cycle-Kiosk-Locations/qd73-bsdg\n",
    "- https://data.austintexas.gov/dataset/Austin-B-Cycle-Trips/tyfh-5r8s\n",
    "\n",
    "(Data is in Public Domain)\n",
    "\n",
    "#### Data Set Headers\n",
    "- B-Cycle Kiosk Locations\n",
    " - kiosk_id\n",
    " - kiosk_name\n",
    " - kiosk_status\n",
    " - latitude\n",
    " - longitude\n",
    " - location\n",
    "- B-Cycle Trips\n",
    " - trip_id\n",
    " - membership_type\n",
    " - bicycle_id\n",
    " - checkout_date\n",
    " - checkout_time\n",
    " - checkout_kiosk_id\n",
    " - checkout_kiosk\n",
    " - return_kiosk_id\n",
    " - return_kiosk\n",
    " - trip_duration_minutes\n",
    " - month\n",
    " - year\n",
    "- B-Cycle Customer Data for Memberships (Added as synthetic data to B-Cycle Trips)\n",
    " - age_range\n",
    " - gender \n",
    " - payment_type\n",
    "\n",
    "### Use Cases \n",
    "Given the dataset, the four analytical use cases are as follows:\n",
    "* What is the average rental duration of each bicycle for a given period? (Used for Predictive Maintenance) \n",
    "* Which bicycle kiosk(s) reaches full capacity, based on real-time streaming of bicycle rental records? (Used for Growth Planning)\n",
    "* How can B-cycle identify less frequently used kiosks to rotate their bicycles so as to even out wear and tear across their fleet? (Used for Preventive Maintenance)\n",
    "* Which membership can be recommended to walk-up customers based on Customer Segmentation of existing members? (Operations Optimization and Sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Preparation Code\n",
    "\n",
    "# Load the two data sets \n",
    "bcycleKiosk = sc.textFile(\"/home/training/assignmentdata/Austin_B-Cycle_Kiosk_Locations.csv\")\n",
    "bcycleTrips = sc.textFile(\"/home/training/assignmentdata/Austin_B-Cycle_Trips.csv\")\n",
    "\n",
    "# Remove Header for bcycleKiosk \n",
    "kioskHeader = bcycleKiosk.first()\n",
    "bcycleKioskNoHeader = bcycleKiosk.filter(lambda line: line != kioskHeader)\n",
    "\n",
    "# Remove Header for bcycleTrips \n",
    "tripHeader = bcycleTrips.first()\n",
    "bcycleTripsNoHeader = bcycleTrips.filter(lambda line: line != tripHeader)\n",
    "\n",
    "# Credit from https://community.hortonworks.com/questions/113255/removing-header-from-csv-file-through-pyspark.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Case 1\n",
    "#### What is the average rental duration of each bicycle for a given period?\n",
    "Application: Predictive Maintenance\n",
    "\n",
    "Description: Wear and tear is inevitable to both bicycles and kiosks. The aim of this use case is to calculate the average rental duration of each bicycle for a given period, with the assumption of a positive correlation between the distance traveled and the rental duration. This will help ground staff identify bicycles that need maintenance before it actually becomes unsafe/unusable (e.g. broken chain, worn brake pads)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(113, ['248', '1040', 4.193548387096774]),\n",
       " (1000, ['307', '1278', 4.162866449511401]),\n",
       " (114, ['330', '1364', 4.133333333333334]),\n",
       " (680, ['299', '1232', 4.120401337792642]),\n",
       " (356, ['361', '1481', 4.1024930747922435])]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assumptions: \n",
    "# 1) To avoid skewing the results,trip duration of < 1 minute will not be considered\n",
    "# 2) Duration of interest: 2016\n",
    "# 3) Expected Output Format: (bicycle_id, number_of_trips, total_distance_traveled, avg_rental_duration)\n",
    "# 4) Scenario A: Find the top 5 highest average rental duration\n",
    "# 5) Scenario B: Find the bicycle with the most number of trips \n",
    "\n",
    "period = '2016'\n",
    "\n",
    "# Filter only 2016 data, and remove records with empty/null bicycle Id \n",
    "# Extract only required fields: bicycle_id, checkout_date, and trip_duration_minutes fields\n",
    "bcycleTripsNoHeaderIn2016 = bcycleTripsNoHeader.map(lambda line: line.split(\",\")).filter(lambda fields: fields[11] == period).filter(lambda fields: len(fields[2])>0)\n",
    "bcycleTripPerBikeDate = bcycleTripsNoHeaderIn2016.filter(lambda fields: int(fields[9]) > 0).map(lambda fields:((fields[2],fields[3]),fields[9]))\n",
    "\n",
    "# Find the total trip duration per bike, per day\n",
    "bcycleTripPerBikeDateTotalTrip = bcycleTripPerBikeDate.reduceByKey(lambda v1, v2: int(v1) + int(v2))\n",
    "\n",
    "# FlatMapValue the RDD to create two records per trip: (bike_id, checkout_date) and (bike_id,trip_duration_minutes)\n",
    "bcycleTripPerBikeDateTotalTripKeyByBikeDate = bcycleTripPerBikeDateTotalTrip.flatMapValues(lambda x : str(x)).map(lambda line: (line[0][0],[line[0][1],line[1]]))\n",
    "bcycleTripPerBikeFlatMap = bcycleTripPerBikeDateTotalTripKeyByBikeDate.flatMapValues(lambda x: x)\n",
    "tupleBikeDate = bcycleTripPerBikeFlatMap.filter(lambda line: \"/\" in line[1])\n",
    "tupleBikeDuration = bcycleTripPerBikeFlatMap.filter(lambda line: \"/\" not in line[1])\n",
    "\n",
    "# Sum the total trip duration\n",
    "tupleBikeSumDurationKeyBikeId = tupleBikeDuration.reduceByKey(lambda v1, v2: str(int(v1) + int(v2)))\n",
    "\n",
    "# Count the total number of trips \n",
    "tupleBikeSumTripsKeyBikeId = tupleBikeDate.map(lambda fields: (fields[0],1)).reduceByKey(lambda v1, v2: str(int(v1) + int(v2)))\n",
    "\n",
    "# Produce the Expected Output Format: (bike_id, number_of_trips, total_trip_duration, avg_rental_duration)\n",
    "expectedOutputFormat = tupleBikeSumTripsKeyBikeId.join(tupleBikeSumDurationKeyBikeId).map(lambda x: (int(x[0]),[x[1][0],x[1][1],int(x[1][1])/float(x[1][0])]))\n",
    "\n",
    "# Scenario A: Find the top 5 highest average rental duration\n",
    "expectedOutputFormat.sortBy((lambda fields: fields[1][2]),False).take(5)\n",
    "\n",
    "# Scenario B: Find the bicycle with the most number of trips \n",
    "# expectedOutputFormat.sortBy((lambda fields: fields[1][0]),False).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Case 2\n",
    "#### Which bicycle kiosk(s) reaches full capacity, based on real-time streaming of bicycle rental records?\n",
    "Application: Growth Planning\n",
    "\n",
    "Description: Bicycle kiosks with no empty space means customers have to cycle further to find another one. This results in lower customer satisfaction and return rate. The aim of this use case is to identify bicycle kiosks that experience full occupancy rates so as to help the operations team focus their upgrading/expansion efforts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Assumptions: \n",
    "# 1) Each kiosk can hold up to 13 bicycles\n",
    "# 2) Each kiosk starts with 8 existing bikes, and 5 vacancies\n",
    "# 3) Duration of interest: Records will stream in for the period of 1 January 2017 to 31 March 2017\n",
    "# 4) Every checkout kiosk will +1 vacancy, and checkin kiosk will -1 vacancy, bounded at 13 and 0 respectively \n",
    "# 5) Kiosks that reach full capacity (i.e. zero vacancies) will have their kiosk ID displayed via streaming\n",
    "\n",
    "# Prepare the data - extract B-cycle data from the period of 1 January 2017 to 31 March 2017, and sort it based on timestamp\n",
    "JanMar2017Trip = bcycleTripsNoHeader.map(lambda line: line.split(',')).filter(lambda fields: fields[11] == '2017').filter(lambda fields: fields[10] in ('1','2','3'))\n",
    "\n",
    "# Prepare the data - pad the timestamp to be all double digits \n",
    "def padTime (fields):\n",
    "    if len(fields[4]) == 7:\n",
    "        fields[4] = '0' + fields[4]\n",
    "    return fields\n",
    "\n",
    "JanMar2017TripPaddedTime = JanMar2017Trip.map(padTime)\n",
    "\n",
    "# Prepare the data - combine date and time fields to form a single datetime stamp key, and sort by it\n",
    "JanMar2017TripSorted = JanMar2017TripPaddedTime.map(lambda fields: ((fields[3],fields[4]),fields)).sortByKey()\n",
    "\n",
    "# Prepare the data - Combine the string tokens into a single string, set encoding to ignore ascii (to avoid printing u'text'), and save it.\n",
    "JanMar2017TripData = JanMar2017TripSorted.map(lambda fields: ','.join(fields[1])).coalesce(1)\n",
    "JanMar2017TripData.map(lambda line: line.encode('ascii', 'ignore')).saveAsTextFile('/home/training/assignmentdata/streamingFolder/trip')\n",
    "\n",
    "# Prepare the data - Create kiosk capacity list - extract name and id of active stations/kiosks, and set capacity to 5\n",
    "bcycleKioskKioskCapacity = bcycleKioskNoHeader.map(lambda line: line.split(',')).filter(lambda fields: fields[2] == 'active').map(lambda fields: (fields[0],fields[1],str(5))).map(lambda fields: ','.join(fields)).coalesce(1)\n",
    "bcycleKioskKioskCapacity.saveAsTextFile('/home/training/assignmentdata/streamingFolder/kiosk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2017-09-01 11:14:51\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-09-01 11:14:54\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-09-01 11:14:57\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-09-01 11:15:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-09-01 11:15:03\n",
      "-------------------------------------------\n",
      "B-Cycle kiosk 2499 has reached maximum capacity\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-09-01 11:15:06\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-09-01 11:15:09\n",
      "-------------------------------------------\n",
      "B-Cycle kiosk 2711 has reached maximum capacity\n",
      "B-Cycle kiosk 2503 has reached maximum capacity\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-09-01 11:15:12\n",
      "-------------------------------------------\n",
      "B-Cycle kiosk 2711 has reached maximum capacity\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-09-01 11:15:15\n",
      "-------------------------------------------\n",
      "B-Cycle kiosk 2498 has reached maximum capacity\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-09-01 11:15:18\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-09-01 11:15:21\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-09-01 11:15:24\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# FOR STREAMING KIOSK DATA\n",
    "# Pre-requisite: Start the process processTrips.py in terminal before starting this cell\n",
    "\n",
    "# Prepare the Spark Streaming Context\n",
    "from pyspark.streaming import StreamingContext\n",
    "ssc = StreamingContext(sc, 3)\n",
    "\n",
    "# Create a SocketTextStream DStream to listen to the port\n",
    "from pyspark import StorageLevel\n",
    "lines = ssc.socketTextStream(\"127.0.0.1\", 8586, StorageLevel.MEMORY_ONLY)\n",
    "\n",
    "# Print the kiosks that are full, based on the messages sent over the socket \n",
    "lines.pprint()\n",
    "\n",
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Stop the Spark Streaming, but keep-alive the Spark Context\n",
    "ssc.stop(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Case 3\n",
    "#### How can B-cycle identify most/less frequently used kiosks to rotate their bicycles so as to even out wear and tear across their fleet?\n",
    "Application: Preventive Maintenance\n",
    "\n",
    "Description: If most rental usage is focused on a small subset of bicycles, their overall lifespan will be greatly reduced. This translates to poor customer experience, and higher maintenance costs. The aim of this use case is to identify kiosks with low levels of usage, and rotate their bicycles with those at kiosks with high levels of usage. This will help to even out the rental usage across the entire fleet of bicycles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+--------------------+\n",
      "|kiosk_id_least_activity|          kiosk_name|\n",
      "+-----------------------+--------------------+\n",
      "|                   2561|State Capitol Vis...|\n",
      "|                   2823|Capital Metro HQ ...|\n",
      "|                   3381|East 7th & Pleasa...|\n",
      "|                   3293|East 2nd & Pedern...|\n",
      "+-----------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assumptions\n",
    "# 1) In order to achieve preventive maintenance, we will first identify the top and bottom 3 kiosks, based on rental activity.\n",
    "#    Bicycles from these two groups should then be rotated \n",
    "# 2) Duration of interest: March 2016\n",
    "# 3) The analysis of each month is independent of the preceding months \n",
    "# 4) Expected Output Format: (most_active_kiosk_id, most_active_kiosk_name, least_active_kiosk_id, least_active_kiosk_name)\n",
    "# 5) Scenario A: It is 31 March 2016. The company wants to know which kiosks' bicycles they should rotate on 1 April 2016\n",
    "\n",
    "# Firstly, create the Schema for the Trips Data Frame \n",
    "from pyspark.sql.types import *\n",
    "trip_schema = StructType( [\n",
    "    StructField('trip_id', StringType(), True),\n",
    "    StructField('membership_type', StringType(), True),\n",
    "    StructField('bicycle_id', StringType(), True),\n",
    "    StructField('checkout_date', StringType(), True),\n",
    "    StructField('checkout_time', StringType(), True),\n",
    "    StructField('checkout_kiosk_id', StringType(), True),\n",
    "    StructField('checkout_kiosk', StringType(), True),\n",
    "    StructField('return_kiosk_id', StringType(), True),\n",
    "    StructField('return_kiosk', StringType(), True),\n",
    "    StructField('trip_duration_minutes', StringType(), True),\n",
    "    StructField('month', StringType(), True),\n",
    "    StructField('year', StringType(), True) ])\n",
    "\n",
    "kiosk_schema = StructType( [\n",
    "    StructField('kiosk_id', StringType(), True),\n",
    "    StructField('kiosk_name', StringType(), True),\n",
    "    StructField('kiosk_status', StringType(), True),\n",
    "    StructField('latitude', StringType(), True),\n",
    "    StructField('longitude', StringType(), True),\n",
    "    StructField('location', StringType(), True) ])\n",
    "\n",
    "# Create the Data Frame for Trips and Kiosk\n",
    "trips = spark.read.csv(\"/home/training/assignmentdata/Austin_B-Cycle_Trips.csv\", header=False, nullValue=\"na\", schema=trip_schema)\n",
    "kiosks = spark.read.csv(\"/home/training/assignmentdata/Austin_B-Cycle_Kiosk_Locations.csv\", header=False, nullValue=\"na\", schema=kiosk_schema)\n",
    "\n",
    "# Filter only records from March 2016 \n",
    "trips2016 = trips.filter('year == 2016').filter('month == 3')\n",
    "\n",
    "# Count the number of checkin and checkout activities that each kiosk experienced\n",
    "trips2016Checkout = trips2016.groupBy('checkout_kiosk_id').agg({'checkout_kiosk_id': 'count'})\n",
    "trips2016Checkin = trips2016.groupBy('return_kiosk_id').agg({'return_kiosk_id': 'count'})\n",
    "\n",
    "# Get top 3 kiosks for checkout/checkin activity\n",
    "top3Checkout = trips2016Checkout.orderBy('count(checkout_kiosk_id)', ascending = False).limit(3)\n",
    "top3Checkin = trips2016Checkin.orderBy('count(return_kiosk_id)', ascending = False).limit(3)\n",
    "\n",
    "# Get bottom 3 kiosks for checkout/checkin activity\n",
    "bottom3Checkout = trips2016Checkout.orderBy('count(checkout_kiosk_id)', ascending = True).limit(3)\n",
    "bottom3Checkin = trips2016Checkin.orderBy('count(return_kiosk_id)', ascending = True).limit(3)\n",
    "\n",
    "# Show the distinct kiosks that have the LEAST checkout/checkin activity \n",
    "bottom3Union = bottom3Checkin.unionAll(bottom3Checkout).select('return_kiosk_id').selectExpr('return_kiosk_id as kiosk_id_least_activity').distinct()\n",
    "bottom3Union.join(kiosks, bottom3Union.kiosk_id_least_activity == kiosks.kiosk_id).select('kiosk_id_least_activity', \"kiosk_name\").show()\n",
    "\n",
    "# 2561 - State Capitol Visitors Garage @ San Jacinto & 12th\n",
    "# 2823 - Capital Metro HQ - East 5th at Broadway\n",
    "# 3381 - East 7th & Pleasant Valley --> closed in 2017\n",
    "# 3293 - East 2nd & Pedernales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+--------------------+\n",
      "|kiosk_id_most_activity|          kiosk_name|\n",
      "+----------------------+--------------------+\n",
      "|                  2498|Convention Center...|\n",
      "|                  2563|Davis at Rainey S...|\n",
      "|                  2539|Convention Center...|\n",
      "+----------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the distinct kiosks that have the MOST checkout/checkin activity \n",
    "top3Union = top3Checkin.unionAll(top3Checkout).select('return_kiosk_id').selectExpr('return_kiosk_id as kiosk_id_most_activity').distinct()\n",
    "top3Union.join(kiosks, top3Union.kiosk_id_most_activity == kiosks.kiosk_id).select('kiosk_id_most_activity', \"kiosk_name\").show()\n",
    "\n",
    "# 2498 - Convention Center / 4th St. @ MetroRail\n",
    "# 2563 - Davis at Rainey Street\n",
    "# 2539 - Convention Center / 3rd & Trinity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Case 4\n",
    "#### Which membership can be recommended to walk-up customers based on Customer Segmentation of existing members?\n",
    "Application: Operations Optimization, Sales Growth\n",
    "\n",
    "Description: There are two main categories of customers - those with pre-paid memberships, and walk-up customers. Based on customer segmentation of existing customers from membership purchases, recommend memberships to walk-up customers based on their age, gender, payment type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Local365', 'Local365', 'Local30', 'Local365', 'Local365', 'Local365', 'Local365', 'Local365', 'Local365', 'Weekender']\n"
     ]
    }
   ],
   "source": [
    "# Assumptions\n",
    "# 1) Synthetic customer data is reflective of membership trends, and are critical fields for customer segmentation\n",
    "# 2) Duration of interest: 2016\n",
    "# 3) The analysis outcome will be used to influence walk-up customers in 2016\n",
    "# 4) Scenario A: Based on customer segmentation of 2016, the company wants to send a personalized message to\n",
    "#    recommend a walk-up customer to purchase a membership that best suits him/her, based on the age, gender, \n",
    "#    and trip duration\n",
    "\n",
    "# Reference: \n",
    "# - https://spark.apache.org/docs/latest/mllib-clustering.html#k-means\n",
    "# - https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.clustering.KMeansModel\n",
    "# - http://vargas-solar.com/big-data-analytics/hands-on/k-means-with-spark-hadoop/\n",
    "\n",
    "# Load the two data sets \n",
    "bcycleCust = sc.textFile(\"/home/training/assignmentdata/Austin_B-Cycle_Trips_Cust.csv\")\n",
    "\n",
    "# Remove Header for bcycleKiosk \n",
    "custHeader = bcycleCust.first()\n",
    "bcycleCustNoHeader = bcycleCust.filter(lambda line: line != custHeader)\n",
    "\n",
    "# Filter only 2016 records \n",
    "bcycleCust2016 = bcycleCustNoHeader.map(lambda line: line.split(',')).filter(lambda fields: fields[11] == '2016')\n",
    "\n",
    "# Extract fields of interest: membership_type, age_range, gender, payment_type\n",
    "bcycleCust2016Fields = bcycleCust2016.map(lambda fields: [fields[1],fields[12],fields[13],fields[14]])\n",
    "\n",
    "# Exclude the walk-up members - cause we want to sell membership to them! \n",
    "bcycleCust2016Members = bcycleCust2016Fields.filter(lambda fields: fields[0] != 'Walk Up')\n",
    "\n",
    "# Python function to convert from predefined string to int \n",
    "def preprocessClustingData(fields):\n",
    "    membershipInt = 0;\n",
    "    ageInt = 0;\n",
    "    genderInt = 0;\n",
    "    paymentInt = 0;\n",
    "    \n",
    "    # Convert membership string\n",
    "    if (fields[0] == 'Walk Up'):\n",
    "        membershipInt = 0.0\n",
    "    elif (fields[0] == 'Weekender'): \n",
    "        membershipInt = 1.0\n",
    "    elif (fields[0] == 'Local30'): \n",
    "        membershipInt = 2.0\n",
    "    elif (fields[0] == 'Local365'): \n",
    "        membershipInt = 3.0\n",
    "    else: \n",
    "        membershipInt = 4.0\n",
    "        \n",
    "    # Convert age string\n",
    "    if (fields[1] == 'below 21'):\n",
    "        ageInt = 0.0\n",
    "    elif (fields[1] == '21 to 35'): \n",
    "        ageInt = 1.0\n",
    "    elif (fields[1] == '36 to 50'): \n",
    "        ageInt = 2.0\n",
    "    elif (fields[1] == '51 to 65'): \n",
    "        ageInt = 3.0\n",
    "    elif (fields[1] == 'above 65'): \n",
    "        ageInt = 4.0\n",
    "    \n",
    "    # Convert gender string\n",
    "    if (fields[2] == 'male'):\n",
    "        genderInt = 0.0\n",
    "    else: \n",
    "        genderInt = 1.0\n",
    "    \n",
    "    # Convert payment string\n",
    "    if (fields[3] == 'visa'):\n",
    "        paymentInt = 0.0\n",
    "    elif (fields[3] == 'mastercard'): \n",
    "        paymentInt = 1.0\n",
    "    elif (fields[3] == 'amex'): \n",
    "        paymentInt = 2.0\n",
    "    elif (fields[3] == 'unionpay'): \n",
    "        paymentInt = 3.0\n",
    "    elif (fields[3] == 'others'): \n",
    "        paymentInt = 4.0\n",
    "\n",
    "    return [membershipInt,ageInt,genderInt,paymentInt]\n",
    "        \n",
    "# Preprocess the data - convert from string to int \n",
    "bcycleCust2016FieldsPreprocessed = bcycleCust2016Members.map(preprocessClustingData)\n",
    "\n",
    "# Build the model (cluster the data)\n",
    "from pyspark.mllib.clustering import KMeans\n",
    "model = KMeans.train(bcycleCust2016FieldsPreprocessed, 10, maxIterations=20, initializationMode='random')\n",
    "\n",
    "# Print the center of the clusters \n",
    "clusterMembership = []\n",
    "for cluster in model.clusterCenters:\n",
    "    recommended = round(cluster[0])\n",
    "    if (recommended == 1.0):\n",
    "        clusterMembership.append('Weekender')\n",
    "    elif (recommended == 2.0):\n",
    "        clusterMembership.append('Local30')\n",
    "    else: \n",
    "        clusterMembership.append('Local365')\n",
    "\n",
    "print clusterMembership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 1.0, 1.0, 4.0] --> Local30\n",
      "[0.0, 2.0, 1.0, 0.0] --> Weekender\n",
      "[0.0, 3.0, 0.0, 2.0] --> Weekender\n",
      "[0.0, 0.0, 1.0, 3.0] --> Local30\n",
      "[0.0, 3.0, 1.0, 1.0] --> Weekender\n",
      "[0.0, 0.0, 1.0, 2.0] --> Weekender\n",
      "[0.0, 1.0, 0.0, 2.0] --> Weekender\n",
      "[0.0, 3.0, 0.0, 2.0] --> Weekender\n",
      "[0.0, 4.0, 1.0, 2.0] --> Local365\n",
      "[0.0, 2.0, 0.0, 0.0] --> Weekender\n",
      "[0.0, 4.0, 0.0, 1.0] --> Local365\n",
      "[0.0, 1.0, 0.0, 1.0] --> Weekender\n",
      "[0.0, 0.0, 0.0, 4.0] --> Local30\n",
      "[0.0, 4.0, 0.0, 3.0] --> Local365\n",
      "[0.0, 0.0, 0.0, 1.0] --> Weekender\n",
      "[0.0, 1.0, 0.0, 1.0] --> Weekender\n",
      "[0.0, 0.0, 0.0, 4.0] --> Local30\n",
      "[0.0, 0.0, 1.0, 3.0] --> Local30\n",
      "[0.0, 0.0, 0.0, 0.0] --> Weekender\n",
      "[0.0, 3.0, 0.0, 1.0] --> Weekender\n",
      "[0.0, 1.0, 0.0, 1.0] --> Weekender\n",
      "[0.0, 2.0, 1.0, 4.0] --> Local30\n",
      "[0.0, 4.0, 1.0, 0.0] --> Local365\n",
      "[0.0, 4.0, 1.0, 2.0] --> Local365\n",
      "[0.0, 3.0, 1.0, 0.0] --> Weekender\n",
      "[0.0, 0.0, 1.0, 1.0] --> Weekender\n",
      "[0.0, 0.0, 0.0, 3.0] --> Local30\n",
      "[0.0, 4.0, 1.0, 3.0] --> Local365\n",
      "[0.0, 4.0, 0.0, 1.0] --> Local365\n",
      "[0.0, 3.0, 0.0, 0.0] --> Weekender\n",
      "[0.0, 2.0, 0.0, 3.0] --> Local30\n",
      "[0.0, 1.0, 0.0, 0.0] --> Weekender\n",
      "[0.0, 0.0, 1.0, 4.0] --> Local30\n",
      "[0.0, 3.0, 1.0, 0.0] --> Weekender\n",
      "[0.0, 1.0, 1.0, 2.0] --> Weekender\n",
      "[0.0, 0.0, 0.0, 1.0] --> Weekender\n",
      "[0.0, 4.0, 1.0, 1.0] --> Local365\n",
      "[0.0, 2.0, 0.0, 2.0] --> Weekender\n",
      "[0.0, 3.0, 0.0, 3.0] --> Local30\n",
      "[0.0, 1.0, 1.0, 0.0] --> Weekender\n",
      "[0.0, 3.0, 0.0, 3.0] --> Local30\n",
      "[0.0, 2.0, 0.0, 4.0] --> Local30\n",
      "[0.0, 0.0, 1.0, 2.0] --> Weekender\n",
      "[0.0, 0.0, 1.0, 1.0] --> Weekender\n",
      "[0.0, 3.0, 1.0, 1.0] --> Weekender\n",
      "[0.0, 4.0, 1.0, 4.0] --> Local365\n",
      "[0.0, 4.0, 1.0, 3.0] --> Local365\n",
      "[0.0, 3.0, 0.0, 2.0] --> Weekender\n",
      "[0.0, 1.0, 0.0, 2.0] --> Weekender\n",
      "[0.0, 3.0, 0.0, 2.0] --> Weekender\n",
      "[0.0, 1.0, 0.0, 2.0] --> Weekender\n",
      "[0.0, 3.0, 0.0, 1.0] --> Weekender\n",
      "[0.0, 3.0, 0.0, 1.0] --> Weekender\n",
      "[0.0, 0.0, 1.0, 0.0] --> Weekender\n",
      "[0.0, 1.0, 1.0, 4.0] --> Local30\n",
      "[0.0, 0.0, 0.0, 0.0] --> Weekender\n",
      "[0.0, 2.0, 0.0, 0.0] --> Weekender\n",
      "[0.0, 0.0, 0.0, 2.0] --> Weekender\n",
      "[0.0, 1.0, 0.0, 0.0] --> Weekender\n",
      "[0.0, 0.0, 0.0, 4.0] --> Local30\n",
      "[0.0, 0.0, 0.0, 1.0] --> Weekender\n",
      "[0.0, 1.0, 0.0, 2.0] --> Weekender\n",
      "[0.0, 3.0, 0.0, 2.0] --> Weekender\n",
      "[0.0, 1.0, 0.0, 0.0] --> Weekender\n",
      "[0.0, 1.0, 1.0, 0.0] --> Weekender\n",
      "[0.0, 0.0, 0.0, 1.0] --> Weekender\n",
      "[0.0, 2.0, 1.0, 4.0] --> Local30\n",
      "[0.0, 1.0, 0.0, 3.0] --> Local30\n",
      "[0.0, 2.0, 1.0, 2.0] --> Weekender\n",
      "[0.0, 4.0, 1.0, 2.0] --> Local365\n",
      "[0.0, 2.0, 0.0, 1.0] --> Weekender\n",
      "[0.0, 2.0, 0.0, 0.0] --> Weekender\n",
      "[0.0, 1.0, 1.0, 3.0] --> Local30\n",
      "[0.0, 4.0, 0.0, 1.0] --> Local365\n",
      "[0.0, 3.0, 1.0, 3.0] --> Local30\n",
      "[0.0, 0.0, 0.0, 2.0] --> Weekender\n",
      "[0.0, 4.0, 0.0, 4.0] --> Local365\n",
      "[0.0, 3.0, 0.0, 2.0] --> Weekender\n",
      "[0.0, 1.0, 0.0, 0.0] --> Weekender\n",
      "[0.0, 4.0, 1.0, 1.0] --> Local365\n",
      "[0.0, 4.0, 1.0, 4.0] --> Local365\n",
      "[0.0, 0.0, 1.0, 1.0] --> Weekender\n",
      "[0.0, 3.0, 0.0, 4.0] --> Local30\n",
      "[0.0, 2.0, 0.0, 2.0] --> Weekender\n",
      "[0.0, 1.0, 1.0, 0.0] --> Weekender\n",
      "[0.0, 1.0, 0.0, 3.0] --> Local30\n",
      "[0.0, 4.0, 1.0, 4.0] --> Local365\n",
      "[0.0, 1.0, 0.0, 3.0] --> Local30\n",
      "[0.0, 3.0, 0.0, 0.0] --> Weekender\n",
      "[0.0, 2.0, 1.0, 3.0] --> Local30\n",
      "[0.0, 0.0, 1.0, 4.0] --> Local30\n",
      "[0.0, 2.0, 0.0, 4.0] --> Local30\n",
      "[0.0, 0.0, 1.0, 4.0] --> Local30\n",
      "[0.0, 2.0, 1.0, 1.0] --> Weekender\n",
      "[0.0, 0.0, 1.0, 4.0] --> Local30\n",
      "[0.0, 3.0, 0.0, 2.0] --> Weekender\n",
      "[0.0, 2.0, 1.0, 3.0] --> Local30\n",
      "[0.0, 2.0, 1.0, 3.0] --> Local30\n",
      "[0.0, 1.0, 0.0, 4.0] --> Local30\n",
      "[0.0, 4.0, 1.0, 0.0] --> Local365\n"
     ]
    }
   ],
   "source": [
    "# Select walk up customers of bcycle in 2016\n",
    "bcycleCust2016WalkUp = bcycleCust2016Fields.filter(lambda fields: fields[0] == 'Walk Up')\n",
    "\n",
    "# Preprocess the data from string to int, and select 10 records for illustration\n",
    "bcycleCust2016WalkUpPreprocessed = bcycleCust2016WalkUp.map(preprocessClustingData)\n",
    "bcycleCust2016WalkUpPreprocessedSubset = bcycleCust2016WalkUpPreprocessed.take(100)\n",
    "\n",
    "for record in bcycleCust2016WalkUpPreprocessedSubset:\n",
    "    cluster = model.predict(record)\n",
    "    print record, '-->', clusterMembership[model.predict(record)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 2.74007577,  3.51680119,  0.49769395,  3.51886015]),\n",
       " array([ 3.09715041,  2.39665745,  0.50054106,  2.32716124]),\n",
       " array([ 1.6015192 ,  1.00882776,  0.49866557,  3.49291727]),\n",
       " array([ 3.19000372,  0.99169044,  0.42143123,  3.8505519 ]),\n",
       " array([ 2.68821109,  3.75784717,  0.49600581,  1.02331962]),\n",
       " array([ 3.23739639,  0.49368121,  0.63011279,  0.37328441]),\n",
       " array([ 2.79381889,  0.49275989,  0.05057273,  1.55716447]),\n",
       " array([ 3.19670871,  0.3943402 ,  0.75253768,  2.53168256]),\n",
       " array([ 2.94327091,  2.38501742,  0.4992378 ,  0.35834059]),\n",
       " array([ 1.37594636,  0.81289206,  0.5299589 ,  0.78628596])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For additional information only \n",
    "model.clusterCenters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
